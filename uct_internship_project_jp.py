# -*- coding: utf-8 -*-
"""UCT Internship Project - JP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FGPconIJc70KwC3VGvNonRcpEzVI4kDa

UCT Internship Project - Smart City

Forecasting of Smart City Traffic Patterns

We are working with the government to transform various cities into a smart city. The vision is to convert it into a digital and intelligent city to improve the efficiency of services for the citizens. One of the problems faced by the government is traffic. You are a data scientist working to manage the traffic of the city better and to provide input on infrastructure planning for the future.

The government wants to implement a robust traffic system for the city by being prepared for traffic peaks. They want to understand the traffic patterns of the four junctions of the city. Traffic patterns on holidays, as well as on various other occasions during the year, differ from normal working days. This is important to take into account for your forecasting.

Dataset Link - https://drive.google.com/file/d/1y61cDyuO9Zrp1fSchWcAmCxk0B6SMx7X/view?usp=sharing

Summary of the Whole Code -

1. Data Loading and Preprocessing:
The code starts by importing required libraries, including data
manipulation (NumPy, Pandas), visualization (Matplotlib, Seaborn), time handling (datetime), and machine learning tools (scikit-learn).
It reads two datasets, dfTrain and dfTest, containing traffic-related data.
The datetime columns are converted to datetime objects for both train and test datasets using pd.to_datetime.

2. Feature Engineering:
New features are created from the datetime information for both train and test datasets. These features include 'Weekday', 'Year', 'Month', 'Day', 'Time', 'Week', and 'Quarter'.
Time-series plots are generated using Seaborn to visualize the relationship between datetime and vehicle count for different junctions.

3. Visualization:
A time-series plot illustrates the variation of vehicle counts over time, with different colors representing different junctions.
Another visualization shows the distribution of vehicle counts across different years for each junction.

4. Feature Transformation:
A function named datetounix1 is defined to convert datetime objects to Unix timestamps (seconds since epoch). This function is applied to both train and test dataframes.

5. Data Preparation:
Data is prepared for modeling by storing predictor features in the 'X' array and the target variable 'Vehicles' in the 'y' array.
One-hot encoding is applied using Pandas' get_dummies function to convert categorical variables into binary columns.

6. Train-Test Split:
The dataset is split into training and testing sets using train_test_split, with a test size of 33% and a specified random seed.

7. Random Forest Regressor:
A Random Forest Regressor model is created with 100 estimators (trees) and a random seed of 42.
The model is trained on the training data using the fit method.

8. Model Evaluation:
Predictions are made on the test set using the trained model.
Evaluation metrics are calculated, including Mean Squared Error (MSE), Mean Absolute Error (MAE), and R-squared (R2) score.
The evaluation metrics are printed to assess the model's performance.

Import Required Libraries
"""

#Importing Required Libraries/Modules
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
import time

from datetime import datetime

from sklearn.model_selection import *
from sklearn.metrics import *
from sklearn.preprocessing import *
from sklearn.ensemble import RandomForestRegressor

#Reading the Datasets
dfTrain=pd.read_csv("train_aWnotuB.csv")
dfTest=pd.read_csv("test_BdBKkAj.csv")

dfTrain.head()

dfTrain.tail()

dfTest.head()

dfTest.tail()

"""Shape of the Datasets"""

print("Shape of Dataset -",dfTrain.shape)
print("Shape of Dataset 2 -",dfTest.shape)

"""Data Info."""

dfTrain.info()

"""Based on the above information we conclude that there are No Null values in any columns of the Train Dataset."""

dfTest.info()

"""Based on the above information we conclude that there are No Null values in any columns of the Test Dataset.

Data Description
"""

#Summary Statistics of the Train Data
dfTrain.describe()

#Summary Statistics of the Test Data
dfTest.describe()

"""Converting DateTime column to DateTime"""

#Converting Data Type to DateTime for both Train and Test Data
dfTrain['DateTime'] = pd.to_datetime(dfTrain['DateTime'])
dfTest['DateTime'] = pd.to_datetime(dfTest['DateTime'])
print("Train Info. -")
dfTrain.info()
print("\n")
print("Test Info. -")
dfTest.info()

"""Feature Engineering

Extracting the Important Features...
"""

##Creating Features from DateTime for Train Data
dfTrain['Weekday'] = [datetime.weekday(date) for date in dfTrain.DateTime]
dfTrain['Year'] = [date.year for date in dfTrain.DateTime]
dfTrain['Month'] = [date.month for date in dfTrain.DateTime]
dfTrain['Day'] = [date.day for date in dfTrain.DateTime]
dfTrain['Time'] = [((date.hour*60+(date.minute))*60)+date.second for date in dfTrain.DateTime]
dfTrain['Week'] = [date.week for date in dfTrain.DateTime]
dfTrain['Quarter'] = [date.quarter for date in dfTrain.DateTime]

#Creating Features from DateTime for Test Data
dfTest['Weekday'] = [datetime.weekday(date) for date in dfTest.DateTime]
dfTest['Year'] = [date.year for date in dfTest.DateTime]
dfTest['Month'] = [date.month for date in dfTest.DateTime]
dfTest['Day'] = [date.day for date in dfTest.DateTime]
dfTest['Time'] = [((date.hour*60+(date.minute))*60)+date.second for date in dfTest.DateTime]
dfTest['Week'] = [date.week for date in dfTest.DateTime]
dfTest['Quarter'] = [date.quarter for date in dfTest.DateTime]

dfTrain

dfTest

"""Drop Duplicate Rows in Train Data"""

#Here, the keep="first" parameter will keep the first occurrence of each duplicated value and removes subsequent occurrences.
#And, the parameter inplace=True will help in changing things directly in the Dataset.
dfTest.drop_duplicates(keep="first", inplace=True)

"""Drop Duplicate Rows in Test Data"""

#Here, the keep="first" parameter will keep the first occurrence of each duplicated value and removes subsequent occurrences.
#And, the parameter inplace=True will help in changing things directly in the Dataset.
dfTest.drop_duplicates(keep="first", inplace=True)

"""Data Visualization"""

#Time-Series Plot
colors = ["cyan","chartreuse","gold","red"]
plt.figure(figsize=(20,4))
time_series=sns.lineplot(x=dfTrain['DateTime'],y="Vehicles",data=dfTrain, hue="Junction", palette=colors)
time_series.set_title("DateTime vs Vehicle")
time_series.set_ylabel("Vehicles in Number")
time_series.set_xlabel("DateTime")
plt.show()

#Years of Traffic at Junction
plt.figure(figsize=(12,5))
colors = ["cyan","chartreuse","gold","red"]
count = sns.countplot(data=dfTrain, x =dfTrain["Year"], hue="Junction", palette=colors)
count.set_title("Years of Traffic at Junctions")
count.set_ylabel("Vehicles in Numbers")
count.set_xlabel("Date")
plt.show()

"""Split the Train data"""

def datetounix1(df):
    #Initialising unixtime list
    unixtime = []

    #Running a loop for converting Date to seconds
    for date in df['DateTime']:
        unixtime.append(time.mktime(date.timetuple()))

    #Replacing Date with unixtime list
    df['DateTime'] = unixtime
    return(df)

train_features = datetounix1(dfTrain.drop(['Vehicles'], axis=1))
test_features = datetounix1(dfTest)


#Store Features/Predictors in an Array
X = train_features
X_valid = test_features

#One Hot Encoding - Using Dummies
X = pd.get_dummies(X)
X_valid = pd.get_dummies(X_valid)

#Store target 'Vehicles' in y array
y = dfTrain['Vehicles'].to_frame()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=512)

"""Random Forest Regressor

Random Forest is an ensemble learning method that builds a collection of decision trees and combines their predictions to produce a more accurate and stable result. It's commonly used for regression and classification tasks.
"""

#Create a Random Forest regressor
rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)

#Train the model
rf_regressor.fit(X_train, y_train)

#Make predictions on the test set
y_pred = rf_regressor.predict(X_test)

#Evaluating the model
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

#Printing the Evaluation Metrics
print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)
print("R2 Score:", r2)

"""Pandas Profiling

This Library refers to a Python package that generates interactive profile reports from a Pandas DataFrame. These reports provide valuable insights and summaries about the data contained within the DataFrame, helping data analysts and scientists to understand their data quickly and efficiently.

The Pandas Profiling library automatically computes various statistics and visualizations for each column in the DataFrame, including:

1. Basic statistics: This includes information like the number of missing values, unique values, mean, median, standard deviation, etc.
2. Common values: The most common values in a column.
3. Histograms: Visual representations of the distribution of data in each column.
4. Correlation matrix: A matrix showing the correlation between numerical columns.
5. Missing values: Visual representation and summary of missing data.
6. Sample of data: A preview of the actual data in the DataFrame.
7. Interactions: Visualizations of interactions between pairs of columns, showing scatter plots or other relevant visualizations.
8. Warnings: Alerts about potential issues in the data, like high cardinality, constant values, etc.
"""

#Pandas Profiling
! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip

from pandas_profiling import ProfileReport

#Profile Report for Train Data
train=ProfileReport(dfTrain)
train

#Profile Report for Test Data
test=ProfileReport(dfTest)
test

